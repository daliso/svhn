{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Nanodegree Capstone\n",
    "\n",
    "This notebook implements the pre-processing needed to generate the input for the convolutional neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, h5py\n",
    "import sys\n",
    "import os\n",
    "import tarfile\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import random\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from skimage.transform import resize\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create directories to organise the files generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = os.getcwd() + \"/source_data/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "temp_directory = os.getcwd() + \"/temp_data/\"\n",
    "if not os.path.exists(temp_directory):\n",
    "    os.makedirs(temp_directory)\n",
    "\n",
    "synthetic_data_directory = os.getcwd() + \"/synthetic_data/\"\n",
    "if not os.path.exists(synthetic_data_directory):\n",
    "    os.makedirs(synthetic_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the SVHN dataset from the Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_hook(t):\n",
    "  \"\"\"\n",
    "  Wraps tqdm instance. Don't forget to close() or __exit__()\n",
    "  the tqdm instance once you're done with it (easiest using `with` syntax).\n",
    "\n",
    "  Example\n",
    "  -------\n",
    "\n",
    "  >>> with tqdm(...) as t:\n",
    "  ...     reporthook = my_hook(t)\n",
    "  ...     urllib.urlretrieve(..., reporthook=reporthook)\n",
    "\n",
    "  \"\"\"\n",
    "  last_b = [0]\n",
    "\n",
    "  def inner(b=1, bsize=1, tsize=None):\n",
    "    \"\"\"\n",
    "    b  : int, optional\n",
    "        Number of blocks just transferred [default: 1].\n",
    "    bsize  : int, optional\n",
    "        Size of each block (in tqdm units) [default: 1].\n",
    "    tsize  : int, optional\n",
    "        Total size (in tqdm units). If [default: None] remains unchanged.\n",
    "    \"\"\"\n",
    "    if tsize is not None:\n",
    "        t.total = tsize\n",
    "    t.update((b - last_b[0]) * bsize)\n",
    "    last_b[0] = b\n",
    "  return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://ufldl.stanford.edu/housenumbers/'\n",
    "\n",
    "    \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  file_save_location = directory + filename\n",
    "  if force or not os.path.exists(file_save_location):\n",
    "    #filename, _ = urlretrieve(url + filename, file_save_location)\n",
    "    with tqdm(unit='B', unit_scale=True, miniters=1,\n",
    "          desc=\"Downloading: \"+filename) as t:  # all optional kwargs\n",
    "        filename, _ = urlretrieve(url + filename, file_save_location, \n",
    "                                  reporthook=my_hook(t), data=None)\n",
    "  statinfo = os.stat(file_save_location)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return file_save_location\n",
    "\n",
    "test_filename = maybe_download('test.tar.gz', 276555967)\n",
    "train_filename = maybe_download('train.tar.gz', 404141560)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Raw SVHN image data\n",
    "\n",
    "If the .tar.gz file downloaded above has not been extracted, then the code below will extract it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(path=directory)\n",
    "    tar.close()\n",
    "  image_names = [d for d in sorted(os.listdir(root))]\n",
    "  return len(image_names) , root\n",
    "  \n",
    "num_train_images , train_folder = maybe_extract(train_filename)\n",
    "num_test_images , test_folder = maybe_extract(test_filename)\n",
    "\n",
    "print(\"Number of training files in [\"+train_folder+\"] = \" + str(num_train_images))\n",
    "print(\"Number of test files in [\"+test_folder+\"] = \" + str(num_test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the bounding box information for the train and test set images\n",
    "\n",
    "The below code is used to process the digitStruct files that come with the SVHN image files. The code extracts the bounding box information for each digit in the image and combines them into a bounding box for a the entire multi-digit house number. It puts these into a dictionary that has the file names as keys and the bounding box information as values. It pickles the dictionary for convenient use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    '''Given two dicts, merge them into a new dict as a shallow copy.'''\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "def calculate_total_bbox(digit_bboxes):\n",
    "    lefts = set()\n",
    "    tops = set()\n",
    "    bottoms = set()\n",
    "    rights = set()\n",
    "    full_number_list = []\n",
    "\t# some code to populate the sets\n",
    "    for bbox in digit_bboxes:\n",
    "\t\tlefts.add(bbox['left'])\n",
    "\t\trights.add(bbox['left']+bbox['width'])\n",
    "\t\tbottoms.add(bbox['top']+bbox['height'])\n",
    "\t\ttops.add(bbox['top'])\n",
    "\t\tfull_number_list.append(str(int(bbox['label'])))\n",
    "    full_number = int(''.join(full_number_list))\n",
    "    return {'left':min(lefts), 'top':min(tops), 'right':max(rights), 'bottom':max(bottoms),'label':full_number}\n",
    "\n",
    "\n",
    "# Credits to https://github.com/hangyao/ for the below class DigitStructFile, I modified it slightly for my needs\n",
    "\n",
    "# The DigitStructFile is just a wrapper around the h5py data.  It basically references \n",
    "#    inf:              The input h5 matlab file\n",
    "#    digitStructName   The h5 ref to all the file names\n",
    "#    digitStructBbox   The h5 ref to all struc data\n",
    "class DigitStructFile:\n",
    "    def __init__(self, inf):\n",
    "        self.inf = h5py.File(inf, 'r')\n",
    "        self.digitStructName = self.inf['digitStruct']['name']\n",
    "        self.digitStructBbox = self.inf['digitStruct']['bbox']\n",
    "\n",
    "# getName returns the 'name' string for for the n(th) digitStruct. \n",
    "    def getName(self,n):\n",
    "        return ''.join([chr(c[0]) for c in self.inf[self.digitStructName[n][0]].value])\n",
    "\n",
    "# bboxHelper handles the coding difference when there is exactly one bbox or an array of bbox. \n",
    "    def bboxHelper(self,attr):\n",
    "        if (len(attr) > 1):\n",
    "            attr = [self.inf[attr.value[j].item()].value[0][0] for j in range(len(attr))]\n",
    "        else:\n",
    "            attr = [attr.value[0][0]]\n",
    "        return attr\n",
    "\n",
    "# getBbox returns a dict of data for the n(th) bbox. \n",
    "    def getBbox(self,n):\n",
    "        bbox = {}\n",
    "        bb = self.digitStructBbox[n].item()\n",
    "        bbox['height'] = self.bboxHelper(self.inf[bb][\"height\"])\n",
    "        bbox['label'] = self.bboxHelper(self.inf[bb][\"label\"])\n",
    "        bbox['left'] = self.bboxHelper(self.inf[bb][\"left\"])\n",
    "        bbox['top'] = self.bboxHelper(self.inf[bb][\"top\"])\n",
    "        bbox['width'] = self.bboxHelper(self.inf[bb][\"width\"])\n",
    "        return bbox\n",
    "\n",
    "    def getDigitStructure(self,n):\n",
    "        s = self.getBbox(n)\n",
    "        s['name']=self.getName(n)\n",
    "        return s\n",
    "# getAllDigitStructure returns all the digitStruct from the input file.     \n",
    "    def getAllDigitStructure(self):\n",
    "        return [self.getDigitStructure(i) for i in range(len(self.digitStructName))]\n",
    "        #return [self.getDigitStructure(i) for i in range(10)]\n",
    "\n",
    "# Return a restructured version of the dataset (one structure by boxed digit).\n",
    "#\n",
    "#   Return a list of such dicts :\n",
    "#      'filename' : filename of the samples\n",
    "#      'boxes' : list of such dicts (one by digit) :\n",
    "#          'label' : 1 to 9 corresponding digits. 10 for digit '0' in image.\n",
    "#          'left', 'top' : position of bounding box\n",
    "#          'width', 'height' : dimension of bounding box\n",
    "#\n",
    "# Note: We may turn this to a generator, if memory issues arise.\n",
    "    def getAllDigitStructure_ByDigit(self):\n",
    "        pictDat = self.getAllDigitStructure()\n",
    "        result = []\n",
    "        structCnt = 1\n",
    "        for i in range(len(pictDat)):\n",
    "            item = { 'filename' : pictDat[i][\"name\"] }\n",
    "            figures = []\n",
    "            for j in range(len(pictDat[i]['height'])):\n",
    "               figure = {}\n",
    "               figure['height'] = pictDat[i]['height'][j]\n",
    "               figure['label']  = pictDat[i]['label'][j]\n",
    "               figure['left']   = pictDat[i]['left'][j]\n",
    "               figure['top']    = pictDat[i]['top'][j]\n",
    "               figure['width']  = pictDat[i]['width'][j]\n",
    "               figures.append(figure)\n",
    "            structCnt = structCnt + 1\n",
    "            item['boxes'] = figures\n",
    "            result.append(item)\n",
    "        result2 = reduce(lambda d1,d2: merge_two_dicts(d1,d2),\n",
    "            map(lambda x: {x['filename']: calculate_total_bbox(x['boxes'])},\n",
    "            result))\n",
    "        return result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maybe_extract_bounding_box_data():\n",
    "  \"\"\"Create the bounding box picle files if not present\"\"\"\n",
    "  if not (os.path.exists(\"./temp_data/test_bboxes.pickle\") and os.path.exists(\"./temp_data/train_bboxes.pickle\")):\n",
    "    # Extract the total bounding box data for the test set and save to a Pickle file for use later\n",
    "    print(\"Starting to extract the bounding box data for the Test set\")\n",
    "    dsf = DigitStructFile(\"./source_data/test/digitStruct.mat\")\n",
    "    bbox_data = dsf.getAllDigitStructure_ByDigit()\n",
    "    print(\"....The number of bounding box records for the Test set is : \"+ str(len(bbox_data)))\n",
    "\n",
    "    set_filename = \"./temp_data/test_bboxes.pickle\"\n",
    "    try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "            pickle.dump(bbox_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "    print(\"....pickled complete Test bbox data set\")\n",
    "\n",
    "\n",
    "    # Extract the total bounding box data for the train set and save to a Pickle file for use later\n",
    "    print(\"Starting to extract the bounding box data for the Train set\")\n",
    "    dsf = DigitStructFile(\"./source_data/train/digitStruct.mat\")\n",
    "    bbox_data = dsf.getAllDigitStructure_ByDigit()\n",
    "    print(\"....The number of bounding box records for the Train set is : \"+ str(len(bbox_data)))\n",
    "\n",
    "    set_filename = \"./temp_data/train_bboxes.pickle\"\n",
    "    try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "            pickle.dump(bbox_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "    print(\"....pickled complete Train bbox data set\")\n",
    "    \n",
    "  else:\n",
    "    print(\"The Digit structs have already been extracted into pickle files for the train and test data\")\n",
    "  return \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maybe_extract_bounding_box_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now use the bounding box data by unpickling the files generated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_bbox_info_file = open(\"./temp_data/test_bboxes.pickle\",'r')\n",
    "test_bboxes_dict = pickle.load(test_bbox_info_file)\n",
    "\n",
    "train_bbox_info_file = open(\"./temp_data/train_bboxes.pickle\",'r')\n",
    "train_bboxes_dict = pickle.load(train_bbox_info_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the raw images\n",
    "\n",
    "Here we open each of the raw images and convert them into ndarrays. Then we do some normalisation of the data. We then crop and scale the images into 54x54 images containing only the housenumbers. We use the bounding box information obtained above to do this cropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 54  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "color_channels = 3 # since we are using color images\n",
    "\n",
    "def crop_and_scale(imagedata,image_filename,bbox_dict):\n",
    "    image_bbox = bbox_dict[image_filename]\n",
    "    \n",
    "    top = int(max(0,image_bbox['top']))\n",
    "    bottom = int(max(0,image_bbox['bottom']))\n",
    "    left = int(max(0,image_bbox['left']))\n",
    "    right = int(max(0,image_bbox['right']))\n",
    "    \n",
    "    width = right - left\n",
    "    height = bottom - top\n",
    "    \n",
    "    top = int(max(0,top-(0.15*height)))\n",
    "    bottom = int(max(0,bottom+(0.15*height)))\n",
    "    left = int(max(0,left-(0.15*width)))\n",
    "    right = int(max(0,right+(0.15*width)))\n",
    "    \n",
    "    cropped = imagedata[top:bottom,left:right,:]\n",
    "    scaled = resize(np.ascontiguousarray(cropped),(64,64))\n",
    "    \n",
    "    randomTop = random.choice(range(10))\n",
    "    randomLeft = random.choice(range(10))\n",
    "    \n",
    "    final_image = scaled[randomTop:54+randomTop,randomLeft:54+randomLeft,:]\n",
    "    return final_image\n",
    "\n",
    "def load_imagadata(folder, bbox_dict):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size, color_channels),\n",
    "                         dtype=np.float32)\n",
    "  labels = np.ndarray(shape=(len(image_files)),dtype=np.int)\n",
    "  image_index = 0\n",
    "  print(\"Processing images in the folder: \" + folder)\n",
    "  for image in os.listdir(folder):\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      cropped_and_scaled = crop_and_scale(image_data,image,bbox_dict)\n",
    "      if cropped_and_scaled.shape != (image_size, image_size, color_channels):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[image_index, :, :,:] = cropped_and_scaled\n",
    "      labels[image_index] = bbox_dict[image]['label']\n",
    "      image_index += 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  num_images = image_index\n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  labels = labels[0:num_images]\n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset, labels\n",
    "\n",
    "test_dataset, test_labels = load_imagadata(test_folder,test_bboxes_dict)\n",
    "train_dataset, train_labels = load_imagadata(train_folder,train_bboxes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at a sample processed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_labels[5])\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.imshow(train_dataset[5]);\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling the processed SVHN dataset for convenient use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_filename = \"./temp_data/test_dataset.pickle\"\n",
    "try:\n",
    "    with open(set_filename, 'wb') as f:\n",
    "        pickle.dump(test_dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', set_filename, ':', e)\n",
    "print(\"pickled test dataset\")\n",
    "\n",
    "set_filename = \"./temp_data/test_labels.pickle\"\n",
    "try:\n",
    "    with open(set_filename, 'wb') as f:\n",
    "        pickle.dump(test_labels, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', set_filename, ':', e)\n",
    "print(\"pickled test labels\")\n",
    "\n",
    "\n",
    "set_filename = \"./temp_data/train_dataset.pickle\"\n",
    "try:\n",
    "    with open(set_filename, 'wb') as f:\n",
    "        pickle.dump(train_dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', set_filename, ':', e)\n",
    "print(\"pickled train dataset\")\n",
    "\n",
    "set_filename = \"./temp_data/train_labels.pickle\"\n",
    "try:\n",
    "    with open(set_filename, 'wb') as f:\n",
    "        pickle.dump(train_labels, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', set_filename, ':', e)\n",
    "print(\"pickled train labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a synthetic dataset for initial training of the neural network\n",
    "\n",
    "We first generate some synthetic raw images using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "        \n",
    "def calculate_font_size(housenumber):\n",
    "    if housenumber >= 10000:\n",
    "        fontsize = 100\n",
    "    elif housenumber >= 1000:\n",
    "        fontsize = 150\n",
    "    elif housenumber >= 100:\n",
    "        fontsize = 200\n",
    "    elif housenumber >= 10:\n",
    "        fontsize = 250\n",
    "    else:\n",
    "        fontsize = 300\n",
    "    return fontsize\n",
    "        \n",
    "def make_housenumber_sample(housenumber_config):\n",
    "    fig = plt.figure(figsize=(5.4, 5.4), dpi=10)\n",
    "    ax = fig.add_subplot(111)\n",
    "    font = FontProperties()\n",
    "    font.set_family(housenumber_config['family'])\n",
    "    font.set_style(housenumber_config['style'])\n",
    "    ax.text(0.5, 0.5, housenumber_config['random_number'],\n",
    "        verticalalignment='center', horizontalalignment='center',\n",
    "        transform=ax.transAxes,\n",
    "        color=housenumber_config['color'], fontsize=calculate_font_size(housenumber_config['random_number']), fontproperties=font, rotation=housenumber_config['rotation'])\n",
    "    ax.axis([0, 54, 0, 54])\n",
    "    plt.axis('off')\n",
    "    #fig.show()\n",
    "    fig.savefig(housenumber_config['filename'],dpi=10)\n",
    "    fig.clf()\n",
    "    plt.close()\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "def generate_synthetic_housenumbers(num_samples=1, fileprefix='shn'):\n",
    "    list_of_hnconfig_tuples = [\n",
    "                        (\n",
    "                            synthetic_data_directory + fileprefix + '/' + fileprefix + '-' + str(x),\n",
    "                            random.choice(['sans-serif','serif']),\n",
    "                            random.choice(['italic','normal']),\n",
    "                            random.choice(['orange','red','blue','black','green']),\n",
    "                            random.choice([0,-20,-10,-5,5,10,20]),\n",
    "                            random.choice(range(random.choice([100,1000,10000,99999])))+1\n",
    "                        ) \n",
    "                        for x in range(num_samples)\n",
    "                        ]\n",
    "    list_of_hnconfig_dicts = map(\n",
    "                            lambda (filename,family,style,color,rotation,random_number):\n",
    "                            {'filename':filename,\n",
    "                             'family':family,\n",
    "                             'style':style,\n",
    "                             'color':color,\n",
    "                             'rotation':rotation,\n",
    "                             'random_number':random_number\n",
    "                            },list_of_hnconfig_tuples\n",
    "                            )\n",
    "    labels = map(\n",
    "                lambda (filename,family,style,color,rotation,random_number):\n",
    "                random_number,\n",
    "                list_of_hnconfig_tuples\n",
    "                )\n",
    "\n",
    "    os.mkdir(synthetic_data_directory+fileprefix)\n",
    "    print(\"Generating images....\")\n",
    "    for hnconfig in list_of_hnconfig_dicts:\n",
    "        #print(hnconfig)\n",
    "        make_housenumber_sample(hnconfig)\n",
    "    make_tarfile(synthetic_data_directory+fileprefix+'.tar.gz',synthetic_data_directory+fileprefix)\n",
    "    \n",
    "    set_filename = synthetic_data_directory+fileprefix+'-labels.pickle'\n",
    "    try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "            pickle.dump(labels, f, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "        \n",
    "    print(\"Done generating \"+str(num_samples)+\" images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maybe_generate_synthetic_images():\n",
    "  \"\"\"Genetate the synthetic datasets used to initially train the neural network\"\"\"\n",
    "  if not (os.path.exists(\"./synthetic_data/synthetic_test_set.tar.gz\") and \n",
    "          os.path.exists(\"./synthetic_data/synthetic_train_set.tar.gz\")):\n",
    "    print(\"Starting to generate synthetic_test_set...\")\n",
    "    start = time.clock() \n",
    "    generate_synthetic_housenumbers(num_samples=15000,fileprefix='synthetic_test_set')\n",
    "    elapsed = time.clock()\n",
    "    elapsed = elapsed - start\n",
    "    print \"Time spent was: \", elapsed\n",
    "\n",
    "    print(\"Starting to generate synthetic_train_set...\")\n",
    "    start = time.clock() \n",
    "    generate_synthetic_housenumbers(num_samples=35000,fileprefix='synthetic_train_set')\n",
    "    elapsed = time.clock()\n",
    "    elapsed = elapsed - start\n",
    "    print \"Time spent is: \", elapsed\n",
    "  else:\n",
    "    print(\"The synthetic images have been created already so skipping generation...\")\n",
    "  print(\"The synthetic images are ready\")\n",
    "  return \"synthetic_train_set\", \"synthetic_test_set\"\n",
    "\n",
    "synthetic_train_folder, synthetic_test_folder =  maybe_generate_synthetic_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now pre-process the synthetic images in the same way we did for the SVHN images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    '''Given two dicts, merge them into a new dict as a shallow copy.'''\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "stestlf = open(synthetic_data_directory+'synthetic_test_set-labels.pickle','r')\n",
    "synthetic_test_labels_list = pickle.load(stestlf)\n",
    "\n",
    "strainlf = open(synthetic_data_directory+'synthetic_train_set-labels.pickle','r')\n",
    "synthetic_train_labels_list = pickle.load(strainlf)\n",
    "\n",
    "synthetic_test_labels_dict = reduce(lambda d1,d2: merge_two_dicts(d1,d2),\n",
    "                                    map(lambda (x,y):{'synthetic_test_set-'+str(x)+'.png':y},\n",
    "                                 zip(range(len(synthetic_test_labels_list)),synthetic_test_labels_list)))\n",
    "\n",
    "synthetic_train_labels_dict = reduce(lambda d1,d2: merge_two_dicts(d1,d2),\n",
    "                                     map(lambda (x,y):{'synthetic_train_set-'+str(x)+'.png':y},\n",
    "                                 zip(range(len(synthetic_train_labels_list)),synthetic_train_labels_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 54  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "color_channels = 3 # since we are using color images\n",
    "\n",
    "def load_synthetic_imagadata(folder,label_dict):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(synthetic_data_directory+folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size, color_channels),\n",
    "                         dtype=np.float32)\n",
    "  labels = np.ndarray(shape=(len(image_files)),dtype=np.int)\n",
    "  image_index = 0\n",
    "  print(\"Processing images in the folder: \" + folder)\n",
    "  for image in sorted(os.listdir(synthetic_data_directory+folder)):\n",
    "    image_file = os.path.join(synthetic_data_directory+folder, image)\n",
    "    try:\n",
    "      img = Image.open(image_file).convert(\"RGB\")\n",
    "      image_data = (np.array(img) - pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size, color_channels):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[image_index, :, :,:] = image_data\n",
    "      labels[image_index] = label_dict[image]\n",
    "      image_index += 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "  num_images = image_index\n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset, labels\n",
    "    \n",
    "synthetic_test_dataset, synthetic_test_labels = load_synthetic_imagadata(synthetic_test_folder, synthetic_test_labels_dict)\n",
    "synthetic_train_dataset, synthetic_train_labels= load_synthetic_imagadata(synthetic_train_folder, synthetic_train_labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling the processed Synthetic dataset for convenient use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_filename = synthetic_data_directory+\"synthetic_test_dataset.pickle\"\n",
    "try:\n",
    "    with open(set_filename, 'wb') as f:\n",
    "        pickle.dump(synthetic_test_dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', set_filename, ':', e)\n",
    "print(\"pickled synthetic test dataset\")\n",
    "\n",
    "set_filename = synthetic_data_directory+\"synthetic_test_labels.pickle\"\n",
    "try:\n",
    "    with open(set_filename, 'wb') as f:\n",
    "        pickle.dump(synthetic_test_labels, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', set_filename, ':', e)\n",
    "print(\"pickled synthetic test labels\")\n",
    "\n",
    "set_filename = synthetic_data_directory+\"synthetic_train_dataset.pickle\"\n",
    "try:\n",
    "    with open(set_filename, 'wb') as f:\n",
    "        pickle.dump(synthetic_train_dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', set_filename, ':', e)\n",
    "print(\"pickled synthetic train dataset\")\n",
    "\n",
    "set_filename = synthetic_data_directory+\"synthetic_train_labels.pickle\"\n",
    "try:\n",
    "    with open(set_filename, 'wb') as f:\n",
    "        pickle.dump(synthetic_train_labels, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', set_filename, ':', e)\n",
    "print(\"pickled synthetic train labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(synthetic_train_labels[10])\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.imshow(synthetic_train_dataset[10]);\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Package all the datasets and labels together into a single collection for further processing by the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_file = open(synthetic_data_directory+'synthetic_train_dataset.pickle','r')\n",
    "source_train_dataset = pickle.load(train_dataset_file)\n",
    "\n",
    "train_labels_file = open(synthetic_data_directory+'synthetic_train_labels.pickle','r')\n",
    "source_train_labels = np.asarray(pickle.load(train_labels_file))\n",
    "\n",
    "test_dataset_file = open(synthetic_data_directory+'synthetic_test_dataset.pickle','r')\n",
    "test_dataset = pickle.load(test_dataset_file)\n",
    "\n",
    "test_labels_file = open(synthetic_data_directory+'synthetic_test_labels.pickle','r')\n",
    "test_labels_1 = np.asarray(pickle.load(test_labels_file))\n",
    "\n",
    "trainlen = int(0.8 * len(source_train_dataset))\n",
    "\n",
    "valid_dataset = source_train_dataset[trainlen:,:,:,:]\n",
    "valid_labels_1 = source_train_labels[trainlen:]\n",
    "\n",
    "train_dataset = source_train_dataset[:trainlen,:,:,:]\n",
    "train_labels_1 = source_train_labels[:trainlen]\n",
    "\n",
    "train_dataset_save = train_dataset\n",
    "train_labels_save = train_labels_1\n",
    "valid_dataset_save = valid_dataset\n",
    "valid_labels_save = valid_labels_1\n",
    "test_dataset_save = test_dataset\n",
    "test_labels_save = test_labels_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'SVHN_synthetic.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset_save,\n",
    "    'train_labels': train_labels_save,\n",
    "    'valid_dataset': valid_dataset_save,\n",
    "    'valid_labels': valid_labels_save,\n",
    "    'test_dataset': test_dataset_save,\n",
    "    'test_labels': test_labels_save,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_file = open(temp_directory+'train_dataset.pickle','r')\n",
    "source_train_dataset = pickle.load(train_dataset_file)\n",
    "\n",
    "train_labels_file = open(temp_directory+'train_labels.pickle','r')\n",
    "source_train_labels = np.asarray(pickle.load(train_labels_file))\n",
    "\n",
    "test_dataset_file = open(temp_directory+'test_dataset.pickle','r')\n",
    "test_dataset = pickle.load(test_dataset_file)\n",
    "\n",
    "test_labels_file = open(temp_directory+'test_labels.pickle','r')\n",
    "test_labels_1 = np.asarray(pickle.load(test_labels_file))\n",
    "\n",
    "trainlen = int(0.8 * len(source_train_dataset))\n",
    "\n",
    "valid_dataset = source_train_dataset[trainlen:,:,:,:]\n",
    "valid_labels_1 = source_train_labels[trainlen:]\n",
    "\n",
    "train_dataset = source_train_dataset[:trainlen,:,:,:]\n",
    "train_labels_1 = source_train_labels[:trainlen]\n",
    "\n",
    "train_dataset_save = train_dataset\n",
    "train_labels_save = train_labels_1\n",
    "valid_dataset_save = valid_dataset\n",
    "valid_labels_save = valid_labels_1\n",
    "test_dataset_save = test_dataset\n",
    "test_labels_save = test_labels_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'SVHN_real.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset_save,\n",
    "    'train_labels': train_labels_save,\n",
    "    'valid_dataset': valid_dataset_save,\n",
    "    'valid_labels': valid_labels_save,\n",
    "    'test_dataset': test_dataset_save,\n",
    "    'test_labels': test_labels_save,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
